{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 20:07:35.134411: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import os\n",
    "from random import seed\n",
    "from model import single_image\n",
    "from unrestricted_advex.eval_kit import logits_to_preds, _validate_logits\n",
    "import mitsuba as mi\n",
    "mi.set_variant('llvm_ad_rgb')\n",
    "import drjit as dr\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import graphviz\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitsuba_bike_scene() -> mi.llvm_ad_rgb.Scene:\n",
    "    bike_scene = mi.load_file('../data_generation/scenes/bike/bike.xml', integrator='prb')\n",
    "    params = mi.traverse(bike_scene)   \n",
    "    params['mat-BodyPaint1_SG.001.reflectance.value'] = [0.15, 0.15, 0.6] # frame, fork\n",
    "    params['mat-GlossBlack_SG.001.reflectance.value'] = [0.0, 0.0, 0.0] # spoke, rims\n",
    "    params['mat-Silver1_SG.001.reflectance.value'] = [0.6, 0.6, 0.6] # chain, gears, rims, cranks\n",
    "    params['mat-CableGray1_SG.001.reflectance.value'] = [0.4, 0.4, 0.4] # cables\n",
    "    params['mat-Wire1_SG.001.reflectance.value'] = [0.1, 0.1, 0.1] # misc. wires\n",
    "    params['mat-Silver2_SG.001.reflectance.value'] = [0.8, 0.8, 0.8] # pedals, handbrake levers\n",
    "    params['mat-TireGom1_SG.001.reflectance.value'] = [0.0, 0.0, 0.0] #tires\n",
    "    params['mat-GlossWhite_SG.001.reflectance.value'] = [0.0, 0.0, 0.0] # handlebars, seat\n",
    "    params['mat-MatWhite1_SG.001.reflectance.value'] = [0.01, 0.01, 0.01] #handlebar covers\n",
    "    params['mat-MatBlack_SG.001.reflectance.value'] = [0.0, 0.0, 0.0] # hand-brakes\n",
    "    params['mat-Material.002.reflectance.value'] = [0.6, 0.6, 0.6]\n",
    "\n",
    "\n",
    "    params.update()             \n",
    "    return bike_scene\n",
    "\n",
    "# orig_bike_scene = mitsuba_bike_scene()\n",
    "# orig_bike_scene_params = mi.traverse(orig_bike_scene)    \n",
    "# orig_bike_image = mi.render(scene=orig_bike_scene, params=orig_bike_scene_params, spp=512)\n",
    "# mi.util.convert_to_bitmap(orig_bike_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitsuba_bird_scene_multi_cam() -> mi.llvm_ad_rgb.Scene:\n",
    "    gray_bird_scene_path = '../data_generation/scenes/multi_cam_bird/gray_bird_multi_light.xml'\n",
    "    gray_bird_scene = mi.load_file(gray_bird_scene_path)\n",
    "    gray_bird_params = mi.traverse(gray_bird_scene)\n",
    "    c = 0.85\n",
    "    wall_color = mi.Color3f(c, c, c)\n",
    "    b = 0.175\n",
    "    bird_color = mi.Color3f(b, b, b)\n",
    "    gray_bird_params['mat-Material.001.reflectance.value'] = bird_color # bike mesh\n",
    "    gray_bird_params['mat-Material.002.reflectance.value'] = wall_color # plane - floor\n",
    "    gray_bird_params['mat-Material.003.reflectance.value'] = wall_color # plane - front\n",
    "    gray_bird_params['mat-Material.004.reflectance.value'] = wall_color # plane - back\n",
    "    gray_bird_params['mat-Material.006.reflectance.value'] = wall_color # plane - left\n",
    "    gray_bird_params['mat-Material.007.reflectance.value'] = wall_color # plane - right\n",
    "    gray_bird_params['Rectangle.emitter.radiance.value'] = [.9, .9, .9] # light rgb\n",
    "    gray_bird_params.update()\n",
    "    return gray_bird_scene\n",
    "\n",
    "# multi_cam_bird_scene = mitsuba_bird_scene_multi_cam() \n",
    "# multi_cam_bird_params = mi.traverse(multi_cam_bird_scene)   \n",
    "# gray_bird_image = mi.render(scene=multi_cam_bird_scene, params=multi_cam_bird_params, sensor = 5, spp=512)\n",
    "# mi.util.convert_to_bitmap(gray_bird_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitsuba_bird_2_scene_multi_cam() -> mi.llvm_ad_rgb.Scene:\n",
    "    gray_bird_scene_path = '../data_generation/scenes/multi_cam_bird_2/gray_bird_2_multi_light.xml'\n",
    "    gray_bird_scene = mi.load_file(gray_bird_scene_path)\n",
    "    gray_bird_params = mi.traverse(gray_bird_scene)\n",
    "    c = 0.85\n",
    "    wall_color = mi.Color3f(c, c, c)\n",
    "    b = 0.175\n",
    "    bird_color = mi.Color3f(b, b, b)\n",
    "    gray_bird_params['mat-12214_bird.reflectance.value'] = bird_color # bike mesh\n",
    "    gray_bird_params['mat-Material.002.reflectance.value'] = wall_color # plane - floor\n",
    "    gray_bird_params['mat-Material.003.reflectance.value'] = wall_color # plane - front\n",
    "    gray_bird_params['mat-Material.004.reflectance.value'] = wall_color # plane - back\n",
    "    gray_bird_params['mat-Material.006.reflectance.value'] = wall_color # plane - left\n",
    "    gray_bird_params['mat-Material.007.reflectance.value'] = wall_color # plane - right\n",
    "    gray_bird_params['Rectangle.emitter.radiance.value'] = [.9, .9, .9] # light rgb\n",
    "    gray_bird_params.update()\n",
    "    return gray_bird_scene\n",
    "\n",
    "# multi_cam_bird_2_scene = mitsuba_bird_2_scene_multi_cam() \n",
    "# multi_cam_bird_2_params = mi.traverse(multi_cam_bird_2_scene)   \n",
    "# gray_bird_2_image = mi.render(scene=multi_cam_bird_2_scene, params=multi_cam_bird_2_params, sensor = 6, spp=512)\n",
    "# mi.util.convert_to_bitmap(gray_bird_2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitsuba_bike_scene_multi_cam() -> mi.llvm_ad_rgb.Scene:\n",
    "    gray_bike_scene_path = '../data_generation/scenes/multi_cam_bike/gray_bike_multi_light.xml'\n",
    "    gray_bike_scene = mi.load_file(gray_bike_scene_path)\n",
    "    gray_bike_params = mi.traverse(gray_bike_scene)\n",
    "    c = 0.85\n",
    "    wall_color = mi.Color3f(c, c, c)\n",
    "    b = 0.2\n",
    "    bike_color = mi.Color3f(b, b, b)\n",
    "    gray_bike_params['mat-default.reflectance.value'] = bike_color # bike mesh\n",
    "    gray_bike_params['mat-Material.002.reflectance.value'] = wall_color # plane - floor\n",
    "    gray_bike_params['mat-Material.003.reflectance.value'] = wall_color # plane - front\n",
    "    gray_bike_params['mat-Material.004.reflectance.value'] = wall_color # plane - back\n",
    "    gray_bike_params['mat-Material.006.reflectance.value'] = wall_color # plane - left\n",
    "    gray_bike_params['mat-Material.007.reflectance.value'] = wall_color # plane - right\n",
    "    gray_bike_params['Rectangle.emitter.radiance.value'] = [.9, .9, .9] # light rgb\n",
    "    gray_bike_params.update()\n",
    "    return gray_bike_scene\n",
    "\n",
    "# multi_cam_bike_scene = mitsuba_bike_scene_multi_cam() \n",
    "# multi_cam_bike_params = mi.traverse(multi_cam_bike_scene)   \n",
    "# gray_bike_image = mi.render(scene=multi_cam_bike_scene, params=multi_cam_bike_params, sensor = 7, spp=512)\n",
    "# mi.util.convert_to_bitmap(gray_bike_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fe1a5608100>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained bird-or-bicycle model\n",
    "checkpoint = torch.load('/nvmescratch/mhull32/unrestricted-adversarial-examples/model_zoo/undefended_pytorch_resnet.pth.tar')\n",
    "model = getattr(models, checkpoint['arch'])(num_classes=2)\n",
    "model = nn.Sequential(nn.BatchNorm2d(num_features=3, affine=False), model)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "target = torch.tensor([1]).cuda() # 0 for bike, 1 for bird\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Image\n",
    "\n",
    "Preprocess Image for Classification\n",
    "\n",
    "Get Model Outputs for the Rendered Bike Scene\n",
    "\n",
    "Calculate the Loss of Model Prediction with Respect to Label\n",
    "\n",
    "Save the Gradient\n",
    "\n",
    "Take the gradient of the loss with respect to the input image and assign it to the _rendered image_ from Mitsuba3\n",
    "\n",
    "Backpropogate the gradient of the loss wrt the input image back through the renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting attack with eps: 0.12549019607843137, alpha: 0.05019607843137255, iters: 5\n",
      "rendering using sensor 6.\n",
      "iter: 0\n",
      "corect: [1.], loss: 0.1603364199399948\n",
      "perturbing vertices...\n",
      "iter: 1\n",
      "corect: [1.], loss: 0.03161793202161789\n",
      "perturbing vertices...\n",
      "iter: 2\n",
      "corect: [1.], loss: 0.031617239117622375\n",
      "perturbing vertices...\n",
      "iter: 3\n",
      "corect: [1.], loss: 0.09854550659656525\n",
      "perturbing vertices...\n",
      "iter: 4\n",
      "corect: [1.], loss: 0.0985506922006607\n",
      "perturbing vertices...\n"
     ]
    }
   ],
   "source": [
    "# keys = ['bike_mesh.vertex_positions']    \n",
    "# keys = ['bird_mesh.vertex_positions']    \n",
    "# keys = ['yellow_bird_mesh.vertex_positions']        \n",
    "# number of iterations * alpha >= epsilon\n",
    "# alpha = epsilon / iters\n",
    "# .006274 = .06274 / 10\n",
    "\n",
    "iters = 5\n",
    "epsilon = 32/255\n",
    "alpha = epsilon / (iters/2)\n",
    "target_mesh_keys = ['yellow_bird_mesh.vertex_positions']\n",
    "# bike_scene = mitsuba_bike_scene_multi_cam()\n",
    "bird_scene = mitsuba_bird_2_scene_multi_cam()\n",
    "\n",
    "def attack_mesh(scene:mi.llvm_ad_rgb.Scene=None, iterations:int=None, epsilon:float=None, alpha:float=None, target_mesh_keys:list=None, **kwargs): \n",
    "    print(f\"starting attack with eps: {epsilon}, alpha: {alpha}, iters: {iterations}\")  \n",
    "    orig_params = mi.traverse(scene)   \n",
    "    orig_mesh_vertex_positions = [dr.llvm.ad.Float(orig_params[k]) for k in target_mesh_keys] \n",
    "    sensor = 0\n",
    "    if 'sensor' in kwargs.keys():\n",
    "        sensor = kwargs['sensor']\n",
    "    else:\n",
    "        pass\n",
    "    print(f'rendering using sensor {sensor}.')\n",
    "    \n",
    "    scene_params = mi.traverse(scene)   \n",
    "    for target_mesh in target_mesh_keys:\n",
    "        dr.enable_grad(scene_params[target_mesh])\n",
    "    scene_params.update()\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        print(f\"iter: {it}\")\n",
    "        image_ref = mi.render(scene, scene_params, sensor=sensor, spp=256, seed=999) \n",
    "        im_path = f'adv_tmp_im_{it}.jpg'   \n",
    "        mi.util.write_bitmap(im_path, image_ref)\n",
    "        time.sleep(1)\n",
    "        im = single_image(path=im_path, resize=299).numpy() \n",
    "        x_np = im.transpose((0, 3, 1, 2))  # from NHWC to NCHW\n",
    "        x = torch.tensor(x_np, requires_grad=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, target).requires_grad_()\n",
    "        loss.backward()\n",
    "        _x_grad = x.grad.detach().clone()\n",
    "        rendered_im_grad = mi.TensorXf(_x_grad.clone().permute(0, 3, 2, 1)[0])\n",
    "        model.zero_grad()\n",
    "        correct = np.equal(logits_to_preds(logits.detach().cpu().numpy()), target.cpu().numpy()).astype(np.float32)\n",
    "        print(f\"corect: {correct}, loss: {loss}\")\n",
    "        if dr.grad_enabled(image_ref) == False:\n",
    "            dr.enable_grad(image_ref)\n",
    "        dr.set_grad(image_ref, rendered_im_grad)\n",
    "        dr.backward(image_ref)  \n",
    "        # attack meshes we are interested in\n",
    "        print(f\"perturbing vertices...\")\n",
    "        for i, k in enumerate(target_mesh_keys):\n",
    "            # the gradient of the output image with respect to the mesh\n",
    "            mesh_grad = dr.grad(scene_params[k])\n",
    "            # update if the gradient is not zero, or skip\n",
    "            if len(np.where(np.array(mesh_grad) > 0)[0]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # l-inf attack, vertex positions + sign of the grad * the learning rate (α)\n",
    "                scene_params[k] = scene_params[k] + dr.sign(mesh_grad) * alpha\n",
    "                delta = scene_params[k] - orig_mesh_vertex_positions[i]\n",
    "                # restrict perturbation to remain within ε budget\n",
    "                delta = dr.clamp(delta, -epsilon, epsilon)\n",
    "                # note: updating the value automatically zeros the grad\n",
    "                scene_params[k] = orig_mesh_vertex_positions[i] + delta\n",
    "            scene_params.update() \n",
    "\n",
    "attack_mesh(scene=bird_scene, iterations=iters, epsilon=epsilon, alpha=alpha, target_mesh_keys=target_mesh_keys, sensor=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Mesh[\n",
      "  name = \"perturbed_bird\",\n",
      "  bbox = BoundingBox3f[\n",
      "    min = [-2.22926, -0.119617, -0.958572],\n",
      "    max = [1.8862, 2.62876, 0.957979]\n",
      "  ],\n",
      "  vertex_count = 1006794,\n",
      "  vertices = [11.5 MiB of vertex data],\n",
      "  face_count = 1994751,\n",
      "  faces = [22.8 MiB of face data],\n",
      "  face_normals = 0\n",
      "], {'faces', 'vertex_positions'})]\n"
     ]
    }
   ],
   "source": [
    "# extract a 'perturbed' mesh\n",
    "# extract and write out a mesh from a scene\n",
    "# use the original .ply to find # vertices, faces\n",
    "\n",
    "p = mi.traverse(bird_scene)\n",
    "mesh = mi.Mesh(\n",
    "    \"perturbed_bird\",\n",
    "    vertex_count=1006794,\n",
    "    face_count=1994752 - 1,\n",
    "    has_vertex_normals=False,\n",
    "    has_vertex_texcoords=False,\n",
    ")\n",
    "mesh_params = mi.traverse(mesh)\n",
    "#print(mesh_params)\n",
    "mesh_params[\"vertex_positions\"] = dr.ravel(p['yellow_bird_mesh.vertex_positions'])\n",
    "mesh_params[\"faces\"] = dr.ravel( p['yellow_bird_mesh.faces'])\n",
    "print(mesh_params.update())\n",
    "mesh.write_ply(\"perturbed_bird.ply\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Show effect of gradients - which meshes receive gradients vs. those that do not, when we backprop classifer loss --> mitsuba  ?\n",
    "    - Visualize by re-render scene heat map to show which meshes have gradient by showing custom material on meshes w/ non-zero grad?\n",
    "    - Visualize by forward propogation of the gradient of the mesh to the output image and then plot gradient image\n",
    "- Does the gradient change when we change camera angle? Insert more sensors into the scene, and then re-run pipeline to watch gradient change\n",
    "- Can achieve expectation over transformation by producing perturbed mesh by rotating camera at different angles and then perturbing the scene based on the gradient produced at various angles?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import scene \n",
    "Attack Loop\n",
    "    Render Scene \n",
    "    Convert to image\n",
    "    Preproccess Image for classification\n",
    "    Get Model Outputs\n",
    "    Calculate Loss of Model Prediction with Respect to Label\n",
    "    Backpropogate the loss through the renderer\n",
    "    Get the Gradient of the loss with respect to the input parameters\n",
    "    Take the sign of the gradient and apply to the input parameter\n",
    "    Update the scene Parameters\n",
    "    Render Scene\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('unr_adv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b1ec98bee90631ae52a8686ce58fef3ae3aa195e31a2dd8aa379804eea27e84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
